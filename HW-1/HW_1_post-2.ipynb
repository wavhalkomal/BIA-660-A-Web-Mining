{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW 1 Analyze Vocabulary and DTM</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: \n",
    "- Please read the problem description carefully\n",
    "- Make sure to complete all requirements (shown as bullets) . In general, it would be much easier if you complete the requirements in the order as shown in the problem description\n",
    "- Submit your codes on Canvas\n",
    "- Sample output is ONLY for your reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze word counts in an input sentence \n",
    "\n",
    "\n",
    "Define a function named `tokenize(text)` which does the following:\n",
    "* accepts a sentence (i.e., `text` parameter) as an input\n",
    "* splits the sentence into a list of tokens by **space** (including tab, and new line). \n",
    "    - e.g., `Hello, my friend!!! How's everything?` will be split into tokens `[\"Hello,\", \"my\",\"friend!!!\",\"How's\",\"everything?\"]`  \n",
    "* removes the **leading/trailing punctuations or spaces** of each token, if any\n",
    "    - e.g., `friedn!!! -> friend`, while `how's` does not change\n",
    "    - hint, you can import module *string*, use `string.punctuation` to get a list of punctuations (say `puncts`), and then use function `strip(puncts)` to remove leading or trailing punctuations in each token\n",
    "* only keeps tokens with 2 or more characters, i.e. `len(token)>1` \n",
    "* converts all tokens into lower case\n",
    "* find the count of each unique token and save the counts as dictionary, i.e., `{hello: 1, my: 1, ...}`\n",
    "* returns the dictionary \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    # enter your code\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lunch': 1,\n",
       " 'school': 1,\n",
       " 'today': 1,\n",
       " 'teacher': 1,\n",
       " 'flying': 1,\n",
       " 'showed': 1,\n",
       " 'homework': 1,\n",
       " 'to': 3,\n",
       " 'la': 1,\n",
       " 'tomorrow': 1,\n",
       " 'my': 1,\n",
       " \"didn't\": 1,\n",
       " 'the': 1,\n",
       " 'now': 1,\n",
       " 'going': 1,\n",
       " 'have': 1,\n",
       " 'am': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code\n",
    "text = \"\"\"I am going to school now. \n",
    "          I didn't have lunch today.\n",
    "          I am flying to LA tomorrow!!!\n",
    "          I showed my homework to the teacher.\"\"\"\n",
    "tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Generate the vocabulary for a list of sentences.\n",
    "\n",
    "- accepts **a list of sentences**, i.e., `sents`, as an input, this will be your corpus\n",
    "- uses `tokenize` function you defined in Q1 to get the count dictionary for each sentence\n",
    "- build a large vocabulary for all sentences (entire corpus), where the keys are the unique words, and the values are the counts for these words in all the sentences.\n",
    "- sort the large dictionary by word count in descending order\n",
    "- return the large vocabulary for this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab (sents):\n",
    "    \n",
    "\n",
    "    # enter your code\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI is considering a significant increase i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ChatGPT subscriptions currently cost $20 per m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's yet unclear whether this would apply mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The move may also be aimed at stabilizing the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anonymous sources told The Information that Op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The subscription term remain unspecified as it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Regardless, this would represent a massive lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The proposed price increase may be tied to Ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>One of these models, codenamed \"Strawberry,\" a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>To achieve this, OpenAI plans to use novel pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Additionally, Strawberry will help generate hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Strawberry could debut as early as this fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OpenAI could face bankruptcy if it doesn't add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Reports from July indicate that the company mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Charging premium prices for cutting-edge AI ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>This situation is particularly ironic given Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The platform now boasts over 200 million weekl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Yet, the business continues to hemorrhage money.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>If OpenAI introduces the rumored $2,000 subscr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Whether that philosophical shift aligns with O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sent\n",
       "0   OpenAI is considering a significant increase i...\n",
       "1   ChatGPT subscriptions currently cost $20 per m...\n",
       "2   It's yet unclear whether this would apply mont...\n",
       "3   The move may also be aimed at stabilizing the ...\n",
       "4   Anonymous sources told The Information that Op...\n",
       "5   The subscription term remain unspecified as it...\n",
       "6   Regardless, this would represent a massive lea...\n",
       "7   The proposed price increase may be tied to Ope...\n",
       "8   One of these models, codenamed \"Strawberry,\" a...\n",
       "9   To achieve this, OpenAI plans to use novel pos...\n",
       "10  Additionally, Strawberry will help generate hi...\n",
       "11      Strawberry could debut as early as this fall.\n",
       "12  OpenAI could face bankruptcy if it doesn't add...\n",
       "13  Reports from July indicate that the company mi...\n",
       "14  Charging premium prices for cutting-edge AI ac...\n",
       "15  This situation is particularly ironic given Ch...\n",
       "16  The platform now boasts over 200 million weekl...\n",
       "17   Yet, the business continues to hemorrhage money.\n",
       "18  If OpenAI introduces the rumored $2,000 subscr...\n",
       "19  Whether that philosophical shift aligns with O..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A test document. \n",
    "\n",
    "sents = pd.read_csv(\"sentences.csv\", encoding='utf-8')\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 14,\n",
       " 'the': 13,\n",
       " 'of': 7,\n",
       " 'could': 7,\n",
       " 'ai': 7,\n",
       " 'that': 6,\n",
       " 'it': 6,\n",
       " 'this': 6,\n",
       " 'for': 6,\n",
       " 'openai': 5,\n",
       " 'models': 5,\n",
       " 'subscription': 4,\n",
       " 'its': 4,\n",
       " 'access': 4,\n",
       " 'be': 4,\n",
       " 'advanced': 4,\n",
       " 'as': 4,\n",
       " 'is': 3,\n",
       " '2,000': 3,\n",
       " 'if': 3,\n",
       " 'from': 3,\n",
       " 'and': 3,\n",
       " 'more': 3,\n",
       " 'strawberry': 3,\n",
       " 'increase': 2,\n",
       " 'premium': 2,\n",
       " 'in': 2,\n",
       " 'month': 2,\n",
       " 'chatgpt': 2,\n",
       " '20': 2,\n",
       " 'per': 2,\n",
       " 'an': 2,\n",
       " 'yet': 2,\n",
       " 'new': 2,\n",
       " 'monthly': 2,\n",
       " 'would': 2,\n",
       " 'whether': 2,\n",
       " 'may': 2,\n",
       " 'company': 2,\n",
       " 'over': 2,\n",
       " 'current': 2,\n",
       " 'benefits': 2,\n",
       " 'language': 2,\n",
       " \"openai's\": 2,\n",
       " 'data': 2,\n",
       " 'users': 2,\n",
       " 'million': 2,\n",
       " 'with': 2,\n",
       " 'shift': 2,\n",
       " 'significant': 1,\n",
       " 'zeros': 1,\n",
       " 'couple': 1,\n",
       " 'fees': 1,\n",
       " 'potentially': 1,\n",
       " 'adding': 1,\n",
       " 'considering': 1,\n",
       " 'cost': 1,\n",
       " 'currently': 1,\n",
       " 'figure': 1,\n",
       " 'rise': 1,\n",
       " 'but': 1,\n",
       " 'subscriptions': 1,\n",
       " 'apply': 1,\n",
       " 'next-gen': 1,\n",
       " 'represents': 1,\n",
       " 'entirely': 1,\n",
       " 'tier': 1,\n",
       " 'capabilities': 1,\n",
       " 'or': 1,\n",
       " 'annually': 1,\n",
       " \"it's\": 1,\n",
       " 'unclear': 1,\n",
       " 'aimed': 1,\n",
       " 'difficulties': 1,\n",
       " 'also': 1,\n",
       " 'rumors': 1,\n",
       " 'move': 1,\n",
       " 'financially': 1,\n",
       " 'amid': 1,\n",
       " 'economic': 1,\n",
       " 'stabilizing': 1,\n",
       " 'at': 1,\n",
       " 'told': 1,\n",
       " 'mulling': 1,\n",
       " 'anonymous': 1,\n",
       " 'information': 1,\n",
       " 'sources': 1,\n",
       " 'unspecified': 1,\n",
       " 'remain': 1,\n",
       " 'yearly': 1,\n",
       " 'range': 1,\n",
       " 'term': 1,\n",
       " 'other': 1,\n",
       " 'features': 1,\n",
       " 'regardless': 1,\n",
       " 'offers': 1,\n",
       " 'which': 1,\n",
       " 'generator': 1,\n",
       " 'assistants': 1,\n",
       " 'priority': 1,\n",
       " 'massive': 1,\n",
       " 'like': 1,\n",
       " 'image': 1,\n",
       " 'custom': 1,\n",
       " 'dall-e': 1,\n",
       " 'represent': 1,\n",
       " 'plus': 1,\n",
       " 'leap': 1,\n",
       " 'designed': 1,\n",
       " 'tackle': 1,\n",
       " 'tied': 1,\n",
       " 'development': 1,\n",
       " 'complex': 1,\n",
       " 'beyond': 1,\n",
       " 'reach': 1,\n",
       " 'tasks': 1,\n",
       " 'powerful': 1,\n",
       " 'reasoning': 1,\n",
       " 'proposed': 1,\n",
       " 'price': 1,\n",
       " 'human-level': 1,\n",
       " 'require': 1,\n",
       " 'handle': 1,\n",
       " 'codenamed': 1,\n",
       " 'traditionally': 1,\n",
       " 'such': 1,\n",
       " 'multi-step': 1,\n",
       " 'problems': 1,\n",
       " 'aims': 1,\n",
       " 'one': 1,\n",
       " 'cognition': 1,\n",
       " 'intricate': 1,\n",
       " 'mathematical': 1,\n",
       " 'these': 1,\n",
       " 'equations': 1,\n",
       " 'feedback': 1,\n",
       " 'themselves': 1,\n",
       " 'refine': 1,\n",
       " 'novel': 1,\n",
       " 'based': 1,\n",
       " 'techniques': 1,\n",
       " 'real-world': 1,\n",
       " 'post-training': 1,\n",
       " 'use': 1,\n",
       " 'plans': 1,\n",
       " 'on': 1,\n",
       " 'human': 1,\n",
       " 'achieve': 1,\n",
       " 'enabling': 1,\n",
       " 'additionally': 1,\n",
       " 'high-quality': 1,\n",
       " 'will': 1,\n",
       " 'help': 1,\n",
       " 'model': 1,\n",
       " 'dubbed': 1,\n",
       " 'orion': 1,\n",
       " 'even': 1,\n",
       " 'generate': 1,\n",
       " 'early': 1,\n",
       " 'fall': 1,\n",
       " 'debut': 1,\n",
       " 'bankruptcy': 1,\n",
       " 'address': 1,\n",
       " 'financial': 1,\n",
       " 'face': 1,\n",
       " 'challenges': 1,\n",
       " \"doesn't\": 1,\n",
       " 'year': 1,\n",
       " 'within': 1,\n",
       " 'months': 1,\n",
       " '12': 1,\n",
       " 'capital': 1,\n",
       " 'run': 1,\n",
       " 'indicate': 1,\n",
       " 'july': 1,\n",
       " 'additional': 1,\n",
       " 'reports': 1,\n",
       " 'raise': 1,\n",
       " 'cash': 1,\n",
       " 'might': 1,\n",
       " 'without': 1,\n",
       " 'billion': 1,\n",
       " 'out': 1,\n",
       " 'prices': 1,\n",
       " 'though': 1,\n",
       " 'demand': 1,\n",
       " 'charging': 1,\n",
       " 'solution': 1,\n",
       " 'reducing': 1,\n",
       " 'risks': 1,\n",
       " 'long-term': 1,\n",
       " 'cutting-edge': 1,\n",
       " 'particularly': 1,\n",
       " \"chatgpt's\": 1,\n",
       " 'ironic': 1,\n",
       " 'success': 1,\n",
       " 'given': 1,\n",
       " 'recent': 1,\n",
       " 'situation': 1,\n",
       " 'attracting': 1,\n",
       " 'businesses': 1,\n",
       " 'active': 1,\n",
       " '200': 1,\n",
       " 'than': 1,\n",
       " 'subscribing': 1,\n",
       " 'weekly': 1,\n",
       " 'tiers': 1,\n",
       " 'now': 1,\n",
       " 'boasts': 1,\n",
       " 'platform': 1,\n",
       " 'continues': 1,\n",
       " 'money': 1,\n",
       " 'business': 1,\n",
       " 'hemorrhage': 1,\n",
       " 'mark': 1,\n",
       " 'reserving': 1,\n",
       " 'rumored': 1,\n",
       " 'major': 1,\n",
       " 'most': 1,\n",
       " 'generative': 1,\n",
       " 'introduces': 1,\n",
       " 'enterprises': 1,\n",
       " 'general': 1,\n",
       " 'seen': 1,\n",
       " 'all': 1,\n",
       " 'intelligence': 1,\n",
       " 'aligns': 1,\n",
       " 'mission': 1,\n",
       " 'stated': 1,\n",
       " 'remains': 1,\n",
       " 'philosophical': 1,\n",
       " 'ensure': 1,\n",
       " 'artificial': 1,\n",
       " 'humanity': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test this function\n",
    "\n",
    "generate_vocab(sents.sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Generate a document term matrix (DTM) as a numpy array\n",
    "\n",
    "\n",
    "Define a function `get_dtm(sents)` as follows:\n",
    "- accepts a list of sentences, i.e., `sents`, as an input\n",
    "- call `tokenize` function you defined in Q1 to get the count dictionary for each sentence, and combine them into a list\n",
    "- call `generate_vocab` function in Q2 to generate the large vocabulary for all sentences, and get all the words, i.e., keys\n",
    "- creates a numpy array, say `dtm` with a shape (# of docs x # of unique words), and set the initial values to 0.\n",
    "- fills cell `dtm[i,j]` with the count of the `j`th word in the `i`th sentence. HINT: you can loop through the list of vocabulary from step 2, and check each word's index in the large vocabulary from step 3, so that you can put the corresponding value into the correct cell. \n",
    "- returns `dtm` and `unique_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtm(sents):\n",
    "    \n",
    "\n",
    "    # enter your code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aimed', 1), ('also', 1), ('amid', 1), ('at', 1), ('be', 1), ('company', 1), ('difficulties', 1), ('economic', 1), ('financially', 1), ('may', 1), ('move', 1), ('of', 1), ('rumors', 1), ('stabilizing', 1), ('the', 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sent    The move may also be aimed at stabilizing the ...\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aimed', 1.0), ('also', 1.0), ('amid', 1.0), ('at', 1.0), ('be', 1.0), ('company', 1.0), ('difficulties', 1.0), ('economic', 1.0), ('financially', 1.0), ('may', 1.0), ('move', 1.0), ('of', 1.0), ('rumors', 1.0), ('stabilizing', 1.0), ('the', 2.0)]\n"
     ]
    }
   ],
   "source": [
    "dtm, all_words = get_dtm(sents.sent)\n",
    "\n",
    "# Check if the array is correct\n",
    "\n",
    "# randomly check one sentence\n",
    "idx = 3\n",
    "\n",
    "# get the dictionary using the function in Q1\n",
    "vocab = tokenize(sents[\"sent\"].loc[idx])\n",
    "print(sorted(vocab.items(), key = lambda item: item[0]))\n",
    "\n",
    "# get all non-zero entries in dtm[idx] and create a dictionary\n",
    "# these two dictionaries should be the same\n",
    "sents.loc[idx]\n",
    "vocab1 ={all_words[j]: dtm[idx][j] for j in np.where(dtm[idx]>0)[0]}\n",
    "print(sorted(vocab1.items(), key = lambda item: item[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Analyze DTM Array \n",
    "\n",
    "\n",
    "**Don't use any loop in this task**. You should use array operations to take the advantage of high performance computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named `analyze_dtm(dtm, words)` which:\n",
    "* takes an array $dtm$ and $words$ as an input, where $dtm$ is the array you get in Q3 with a shape $(m \\times n)$, and $words$ contains an array of words corresponding to the columns of $dtm$.\n",
    "* calculates the sentence frequency for each word, say $j$, e.g. how many sentences contain word $j$. Save the result to array $df$ ($df$ has shape of $(n,)$ or $(1, n)$).\n",
    "* normalizes the word count per sentence: divides word count, i.e., $dtm_{i,j}$, by the total number of words in sentence $i$. Save the result as an array named $tf$ ($tf$ has shape of $(m,n)$).\n",
    "* for each $dtm_{i,j}$, calculates $tf\\_idf_{i,j} = \\frac{tf_{i, j}}{df_j}$, i.e., divide each normalized word count by the sentence frequency of the word. The reason is, if a word appears in most sentences, it does not have the discriminative power and often is called a `stop` word. The inverse of $df$ can downgrade the weight of such words. $tf\\_idf$ has shape of $(m,n)$\n",
    "* prints out the following:\n",
    "    \n",
    "    - the total number of words in the document represented by $dtm$\n",
    "    - the most frequent top 10 words in this document, is the result the same as Q2, if not, why? Please explain in text.\n",
    "    - words with the top 10 largest $df$ values (show words and their $df$ values)\n",
    "    - the longest sentence (i.e., the one with the most words)\n",
    "    - top-10 words with the largest $tf\\_idf$ values in the longest sentence (show words and values) \n",
    "* returns the $tf\\_idf$ array.\n",
    "\n",
    "\n",
    "\n",
    "Note, for all the steps, **do not use any loop**. Just use array functions and broadcasting for high performance computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dtm(dtm, words, sents):\n",
    "    \n",
    "\n",
    "    # enter your code\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words:\n",
      "362.0\n",
      "\n",
      "The top 10 frequent words:\n",
      "[('to', 14.0), ('the', 13.0), ('of', 7.0), ('could', 7.0), ('ai', 7.0), ('that', 6.0), ('it', 6.0), ('this', 6.0), ('for', 6.0), ('models', 5.0)]\n",
      "\n",
      "The top 10 words with highest df values:\n",
      "[('to', 10), ('the', 9), ('could', 7), ('ai', 7), ('this', 6), ('of', 6), ('for', 6), ('it', 6), ('openai', 5), ('that', 5)]\n",
      "\n",
      "The longest sentence :\n",
      "Regardless, this would represent a massive leap from the current $20 per month for ChatGPT Plus, which offers benefits like priority access, custom AI assistants, the DALL-E 3 image generator, and other advanced features.\n",
      "\n",
      "The top 10 words with highest tf-idf values in the longest sentece:\n",
      "[('offers', 0.03125), ('plus', 0.03125), ('other', 0.03125), ('features', 0.03125), ('regardless', 0.03125), ('which', 0.03125), ('generator', 0.03125), ('assistants', 0.03125), ('priority', 0.03125), ('like', 0.03125)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.01111111, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00714286, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00434783, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.01428571, 0.01587302, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.01111111, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00869565, 0.        , 0.00724638, ..., 0.04347826, 0.04347826,\n",
       "        0.04347826]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array(all_words)\n",
    "\n",
    "analyze_dtm(dtm, words, sents.sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus. Find keywords of the document (1 point)\n",
    "Can you leverage  ùëëùë°ùëö\n",
    "  array you generated to find a few keywords that can be used to tag this document? e.g., AI, ChatGPT, etc.\n",
    "\n",
    "Describe your ideas and also implement your ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
